---
title: "Assignment 7 - Machine Learning"
author: "Ken Benoit and Slava Mikhaylov"
output: html_document
---

Assignments for the course focus on practical aspects of the concepts covered in the lectures. Assignments are largely based on the material covered in James et al. (2013). You will start working on the assignment in the lab sessions after the lectures, but may need to finish them after class.

You will be asked to submit your assignments via Moodle by 7pm on the day of the class. We will subsequently open up solutions to the problem sets. 

### Exercise summary

This exercise revisits regression as a "machine learning" tool, both for linear and logisitic regression, as well as simple machine learning using the Naive Bayes and kNN algorithms.   

For this execise we will use the Stata dataset  `dail2002.dta` from the article Kenneth Benoit and Michael Marsh. 2008. "[The Campaign Value of Incumbency: A New Solution to the Puzzle of Less Effective Incumbent Spending.](http://www.kenbenoit.net/pdfs/ajps_348.pdf)" *American Journal of Political Science* 52(4, October): 874-890.  This is available directly  [here](http://www.kenbenoit.net/files/dail2002.dta).  To load this into R, you will need the `read.dta` command from the `foreign` package.  (Note that you can load straight from the URL using this command.)  Call this data object `dail2002`. 
```{r}
require(foreign, quietly = TRUE)
dail2002 <- read.dta("http://www.kenbenoit.net/files/dail2002.dta")
```
    
1.  **Partitioning the dataset into "folds".**  For this exercise, we will be fitting a model from 
    a subset of the data, and using the fitted model to predict the outcomes from the "left out" set 
    and using that to evaluate root mean squared error (RMSE) or accuracy.
    
    1. Use the `sample()` command to draw a random sample of one-fifth of the observations in the `dail2002.dta` dataset.
    
    2. For the categorical predictions, we will also assess predictive ability based on "leave-one-out" testing and using "folds", which are groups of observations that have been left out of the training set, and then attempting to predict them with accuracy.  You will need to use indexing to partition the dataset to carry this out, and you can use the `sample()` function as we did on Friday.

2.  **OLS regression for prediction.**

    1.  Fit a regression from the dataset to predict `votes1st`.  You may use any combination of
        regressors that you wish.  Save the model object to `reg2_1`.
        
    2.  Predict the `votes1st` from the same sample to which you fitted the regression.  What is the 
        Root Mean Squared Error (RMSE) and how would you interpret this?
        
    3.  Drop the incumbency variable -- that you hopefully included in your answer to 2.1! -- and
        repeat steps 2.1--2.2.  Compute a new RMSE and compare this to the previous one.  Which
        is a better predictor?
        
3.  **Logistic regression for prediction**.

    1.  Fit a logistic regression (hint: use `glm()`) to predict the outcome variable `wonseat`.  Use 
        any specification that you think provides a good prediction.
    
    2.  For the full sample, compute:
        
        *  a table of actual `wonseat` by predicted `wonseat`
        *  percent correctly predicted
        *  precision
        *  recall
    
    3.  Comparing two models.
    
        *  Compute an 8-fold validation, where for 8 different training sets consisting of 7/8 of the observations, you predict the other held-out 1/8 and compare the actual to predicted for the 1/8 test set.  Compute an average F1 score for the 8 models.
        
        *  Now drop a variable or two, and repeat the previous step to compare the average F1 score for this model.
        
        *  Why is it valuable to use the different folds here, rather than simply comparing the F1 score for the predicted outcome of the entire sample, fit to the entire sample?
    
    
4.  **kNN prediction.**

    1.  Fit a $k=1$ kNN model to predict the same specification as in your logistic regression.  Compare the 
        percent correctly predicted.  Which model worked better?  (Note: You can use the whole sample here, so that this will compare the results to those from 3.1).
        
    2.  Experiment with two more settings of $k$ to see how this affects prediction, reporting the percent
        correctly predicted.

